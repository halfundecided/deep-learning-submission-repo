{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM2: Numerical Optimization for Logistic Regression.\n",
    "\n",
    "### Name: Mijeong Ban\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read the lecture note: [click here](https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf)\n",
    "\n",
    "2. Read, complete, and run my code.\n",
    "\n",
    "3. **Implement mini-batch SGD** and evaluate the performance.\n",
    "\n",
    "4. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain **the code** and **the output after execution**.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "5. Upload this .HTML file to your Google Drive, Dropbox, or your Github repo.  (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "6. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM2/HM2.html\n",
    "\n",
    "\n",
    "## Grading criteria:\n",
    "\n",
    "1. When computing the ```gradient``` and ```objective function value``` using a batch of samples, use **matrix-vector multiplication** rather than a FOR LOOP of **vector-vector multiplications**.\n",
    "\n",
    "2. Plot ```objective function value``` against ```epochs```. In the plot, compare GD, SGD, and MB-SGD (with $b=8$ and $b=64$). The plot must look reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 8)\n",
      "Shape of x_test: (128, 8)\n",
      "Shape of y_train: (640, 1)\n",
      "Shape of y_test: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = 640\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[ 0.0135995   0.03748734  0.00170308  0.0213963   0.0402135   0.12008365\n",
      "   0.07242613 -0.11349401]]\n",
      "test std = \n",
      "[[1.06218299 0.96452031 1.02361744 1.00808248 0.85159978 0.94991108\n",
      "  1.02622122 0.92742754]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add a dimension of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 9)\n",
      "Shape of x_test: (128, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     objective function value (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "    loss = numpy.mean(vec2) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    return loss + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial objective function value = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initialize w\n",
    "d = x_train.shape[1]\n",
    "w = numpy.zeros((d, 1))\n",
    "\n",
    "# evaluate the objective function value at w\n",
    "lam = 1E-6\n",
    "objval0 = objective(w, x_train, y_train, lam)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ is $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     g: g: d-by-1 matrix, full gradient\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "    vec3 = -numpy.mean(vec2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    g = vec3 + lam * w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each iteration's objective value\n",
    "def grad_descent(x, y, lam, stepsize, max_iter=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_iter) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= stepsize * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at t=0 is 0.6931471805599453\n",
      "Objective value at t=1 is 0.5978060608074112\n",
      "Objective value at t=2 is 0.5566940687955698\n",
      "Objective value at t=3 is 0.5342028076135961\n",
      "Objective value at t=4 is 0.5201060935576933\n",
      "Objective value at t=5 is 0.5105582689871058\n",
      "Objective value at t=6 is 0.5037687689151319\n",
      "Objective value at t=7 is 0.498777014085264\n",
      "Objective value at t=8 is 0.4950160503494915\n",
      "Objective value at t=9 is 0.4921282609722473\n",
      "Objective value at t=10 is 0.4898769694412095\n",
      "Objective value at t=11 is 0.4880997584185422\n",
      "Objective value at t=12 is 0.486681951557817\n",
      "Objective value at t=13 is 0.4855406831732998\n",
      "Objective value at t=14 is 0.48461490254575873\n",
      "Objective value at t=15 is 0.4838588804334873\n",
      "Objective value at t=16 is 0.48323786902794996\n",
      "Objective value at t=17 is 0.48272513098224906\n",
      "Objective value at t=18 is 0.48229986332717234\n",
      "Objective value at t=19 is 0.4819457202900167\n",
      "Objective value at t=20 is 0.4816497452270318\n",
      "Objective value at t=21 is 0.4814015871341734\n",
      "Objective value at t=22 is 0.48119291835347566\n",
      "Objective value at t=23 is 0.481016996635071\n",
      "Objective value at t=24 is 0.4808683321760061\n",
      "Objective value at t=25 is 0.4807424319483554\n",
      "Objective value at t=26 is 0.48063560158337904\n",
      "Objective value at t=27 is 0.4805447905699637\n",
      "Objective value at t=28 is 0.4804674703684106\n",
      "Objective value at t=29 is 0.4804015377637333\n",
      "Objective value at t=30 is 0.4803452377350296\n",
      "Objective value at t=31 is 0.4802971015327527\n",
      "Objective value at t=32 is 0.48025589669224544\n",
      "Objective value at t=33 is 0.48022058647843396\n",
      "Objective value at t=34 is 0.480190296828641\n",
      "Objective value at t=35 is 0.48016428929107313\n",
      "Objective value at t=36 is 0.4801419387832959\n",
      "Objective value at t=37 is 0.48012271524485095\n",
      "Objective value at t=38 is 0.4801061684505855\n",
      "Objective value at t=39 is 0.4800919154004581\n",
      "Objective value at t=40 is 0.48007962981800184\n",
      "Objective value at t=41 is 0.4800690333810212\n",
      "Objective value at t=42 is 0.4800598883802463\n",
      "Objective value at t=43 is 0.48005199155894673\n",
      "Objective value at t=44 is 0.4800451689321841\n",
      "Objective value at t=45 is 0.48003927142100833\n",
      "Objective value at t=46 is 0.4800341711663764\n",
      "Objective value at t=47 is 0.48002975841141987\n",
      "Objective value at t=48 is 0.4800259388600266\n",
      "Objective value at t=49 is 0.4800226314354766\n",
      "Objective value at t=50 is 0.4800197663757613\n",
      "Objective value at t=51 is 0.48001728361279045\n",
      "Objective value at t=52 is 0.4800151313914018\n",
      "Objective value at t=53 is 0.48001326509126985\n",
      "Objective value at t=54 is 0.48001164622076203\n",
      "Objective value at t=55 is 0.48001024155672634\n",
      "Objective value at t=56 is 0.48000902240830406\n",
      "Objective value at t=57 is 0.48000796398628764\n",
      "Objective value at t=58 is 0.4800070448624072\n",
      "Objective value at t=59 is 0.4800062465053321\n",
      "Objective value at t=60 is 0.4800055528821826\n",
      "Objective value at t=61 is 0.4800049501160457\n",
      "Objective value at t=62 is 0.4800044261914124\n",
      "Objective value at t=63 is 0.4800039707006608\n",
      "Objective value at t=64 is 0.4800035746257235\n",
      "Objective value at t=65 is 0.48000323014994367\n",
      "Objective value at t=66 is 0.4800029304958498\n",
      "Objective value at t=67 is 0.48000266978520345\n",
      "Objective value at t=68 is 0.4800024429181969\n",
      "Objective value at t=69 is 0.4800022454691311\n",
      "Objective value at t=70 is 0.4800020735962808\n",
      "Objective value at t=71 is 0.4800019239639819\n",
      "Objective value at t=72 is 0.4800017936752577\n",
      "Objective value at t=73 is 0.48000168021353173\n",
      "Objective value at t=74 is 0.48000158139218224\n",
      "Objective value at t=75 is 0.48000149531086916\n",
      "Objective value at t=76 is 0.4800014203177077\n",
      "Objective value at t=77 is 0.48000135497649726\n",
      "Objective value at t=78 is 0.4800012980383191\n",
      "Objective value at t=79 is 0.48000124841691355\n",
      "Objective value at t=80 is 0.48000120516732675\n",
      "Objective value at t=81 is 0.480001167467388\n",
      "Objective value at t=82 is 0.4800011346016376\n",
      "Objective value at t=83 is 0.4800011059473778\n",
      "Objective value at t=84 is 0.4800010809625628\n",
      "Objective value at t=85 is 0.4800010591752824\n",
      "Objective value at t=86 is 0.4800010401746296\n",
      "Objective value at t=87 is 0.4800010236027646\n",
      "Objective value at t=88 is 0.4800010091480208\n",
      "Objective value at t=89 is 0.4800009965389121\n",
      "Objective value at t=90 is 0.480000985538923\n",
      "Objective value at t=91 is 0.4800009759419801\n",
      "Objective value at t=92 is 0.4800009675685128\n",
      "Objective value at t=93 is 0.48000096026202743\n",
      "Objective value at t=94 is 0.480000953886127\n",
      "Objective value at t=95 is 0.48000094832191825\n",
      "Objective value at t=96 is 0.48000094346575434\n",
      "Objective value at t=97 is 0.4800009392272719\n",
      "Objective value at t=98 is 0.48000093552768\n",
      "Objective value at t=99 is 0.48000093229827295\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 1.0\n",
    "w, objvals_gd = grad_descent(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi # 1-by-d matrix\n",
    "    yxw = float(numpy.dot(yx, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-yxw)) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + numpy.exp(yxw)) # d-by-1 matrix\n",
    "    g = g_loss + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def sgd(x, y, lam, stepsize, max_epoch=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i, :]) # scalar\n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.5263384162833591\n",
      "Objective value at epoch t=1 is 0.534610720451609\n",
      "Objective value at epoch t=2 is 0.5234052813923149\n",
      "Objective value at epoch t=3 is 0.5255673045583448\n",
      "Objective value at epoch t=4 is 0.5185814648267136\n",
      "Objective value at epoch t=5 is 0.5144861913830303\n",
      "Objective value at epoch t=6 is 0.5060597712959412\n",
      "Objective value at epoch t=7 is 0.5037444080754785\n",
      "Objective value at epoch t=8 is 0.4981780473144495\n",
      "Objective value at epoch t=9 is 0.5009323115714958\n",
      "Objective value at epoch t=10 is 0.49841510750634965\n",
      "Objective value at epoch t=11 is 0.49588742711489137\n",
      "Objective value at epoch t=12 is 0.50003468347499\n",
      "Objective value at epoch t=13 is 0.4934072834327239\n",
      "Objective value at epoch t=14 is 0.4935482754197408\n",
      "Objective value at epoch t=15 is 0.4927302449861958\n",
      "Objective value at epoch t=16 is 0.4935764137787957\n",
      "Objective value at epoch t=17 is 0.49001258968407047\n",
      "Objective value at epoch t=18 is 0.49007619547463444\n",
      "Objective value at epoch t=19 is 0.48707099531731846\n",
      "Objective value at epoch t=20 is 0.4863729824508898\n",
      "Objective value at epoch t=21 is 0.48835138029820435\n",
      "Objective value at epoch t=22 is 0.4869304113319394\n",
      "Objective value at epoch t=23 is 0.48631186928738845\n",
      "Objective value at epoch t=24 is 0.48555207620182994\n",
      "Objective value at epoch t=25 is 0.4850076454257257\n",
      "Objective value at epoch t=26 is 0.48410602844021255\n",
      "Objective value at epoch t=27 is 0.4834920441851529\n",
      "Objective value at epoch t=28 is 0.483754838859108\n",
      "Objective value at epoch t=29 is 0.48340271257493395\n",
      "Objective value at epoch t=30 is 0.4829866501460497\n",
      "Objective value at epoch t=31 is 0.4827600406346706\n",
      "Objective value at epoch t=32 is 0.48248685368253835\n",
      "Objective value at epoch t=33 is 0.48233146664958565\n",
      "Objective value at epoch t=34 is 0.48212478093703093\n",
      "Objective value at epoch t=35 is 0.4818623541610066\n",
      "Objective value at epoch t=36 is 0.48171718077751635\n",
      "Objective value at epoch t=37 is 0.4815384412785636\n",
      "Objective value at epoch t=38 is 0.48135647878222854\n",
      "Objective value at epoch t=39 is 0.48124078205313126\n",
      "Objective value at epoch t=40 is 0.4811055225434161\n",
      "Objective value at epoch t=41 is 0.48101881908060673\n",
      "Objective value at epoch t=42 is 0.4808659792093189\n",
      "Objective value at epoch t=43 is 0.480825757211975\n",
      "Objective value at epoch t=44 is 0.4807403431541194\n",
      "Objective value at epoch t=45 is 0.480660109055621\n",
      "Objective value at epoch t=46 is 0.48060629418291134\n",
      "Objective value at epoch t=47 is 0.4805398731918823\n",
      "Objective value at epoch t=48 is 0.48048861903524165\n",
      "Objective value at epoch t=49 is 0.48043605166139863\n",
      "Objective value at epoch t=50 is 0.4803972154689138\n",
      "Objective value at epoch t=51 is 0.4803577625693033\n",
      "Objective value at epoch t=52 is 0.4803211547065863\n",
      "Objective value at epoch t=53 is 0.4802899631444407\n",
      "Objective value at epoch t=54 is 0.4802608060588594\n",
      "Objective value at epoch t=55 is 0.48023499224265576\n",
      "Objective value at epoch t=56 is 0.4802110655062365\n",
      "Objective value at epoch t=57 is 0.48019113176297135\n",
      "Objective value at epoch t=58 is 0.480171468556631\n",
      "Objective value at epoch t=59 is 0.4801548147564576\n",
      "Objective value at epoch t=60 is 0.48013903028889715\n",
      "Objective value at epoch t=61 is 0.480125779540861\n",
      "Objective value at epoch t=62 is 0.48011317059594133\n",
      "Objective value at epoch t=63 is 0.48010220914808943\n",
      "Objective value at epoch t=64 is 0.48009200514222183\n",
      "Objective value at epoch t=65 is 0.48008292386381457\n",
      "Objective value at epoch t=66 is 0.48007474099740677\n",
      "Objective value at epoch t=67 is 0.4800673518178985\n",
      "Objective value at epoch t=68 is 0.4800607689682598\n",
      "Objective value at epoch t=69 is 0.48005482896182816\n",
      "Objective value at epoch t=70 is 0.48004938149628956\n",
      "Objective value at epoch t=71 is 0.48004461225678596\n",
      "Objective value at epoch t=72 is 0.48004024459586636\n",
      "Objective value at epoch t=73 is 0.48003630956562954\n",
      "Objective value at epoch t=74 is 0.48003281499407685\n",
      "Objective value at epoch t=75 is 0.4800296319342978\n",
      "Objective value at epoch t=76 is 0.48002676208142175\n",
      "Objective value at epoch t=77 is 0.48002419964190457\n",
      "Objective value at epoch t=78 is 0.48002187685462594\n",
      "Objective value at epoch t=79 is 0.4800197969728236\n",
      "Objective value at epoch t=80 is 0.48001792414995964\n",
      "Objective value at epoch t=81 is 0.4800162387471126\n",
      "Objective value at epoch t=82 is 0.4800147144521635\n",
      "Objective value at epoch t=83 is 0.4800133505331393\n",
      "Objective value at epoch t=84 is 0.4800121206796515\n",
      "Objective value at epoch t=85 is 0.48001101227020193\n",
      "Objective value at epoch t=86 is 0.48001001335142546\n",
      "Objective value at epoch t=87 is 0.48000911781213923\n",
      "Objective value at epoch t=88 is 0.48000831086351414\n",
      "Objective value at epoch t=89 is 0.480007583392075\n",
      "Objective value at epoch t=90 is 0.480006929345147\n",
      "Objective value at epoch t=91 is 0.4800063409820178\n",
      "Objective value at epoch t=92 is 0.48000581107605605\n",
      "Objective value at epoch t=93 is 0.48000533382641447\n",
      "Objective value at epoch t=94 is 0.4800049046949907\n",
      "Objective value at epoch t=95 is 0.4800045185258634\n",
      "Objective value at epoch t=96 is 0.4800041708429473\n",
      "Objective value at epoch t=97 is 0.4800038579198989\n",
      "Objective value at epoch t=98 is 0.48000357621492895\n",
      "Objective value at epoch t=99 is 0.48000332275959146\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 0.1\n",
    "w, objvals_sgd = sgd(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD with SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd))\n",
    "epochs_sgd = range(len(objvals_sgd))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd, '--b', LineWidth=4)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd, '-r', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1], ['GD', 'SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.2234375\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error_train = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.1953125\n"
     ]
    }
   ],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error_test = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Mini-batch SGD (fill the code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Compute the objective $Q_I$ and its gradient using a batch of samples\n",
    "\n",
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: b-by-d matrix\n",
    "#     yi: b-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def mb_stochastic_objective_gradient(w, xi, yi, lam, b):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of stochastic_objective_gradient\n",
    "    # Use matrix-vector multiplication; do not use FOR LOOP of vector-vector multiplications\n",
    "    d = xi.shape[1]\n",
    "    yx = numpy.multiply(yi, xi)\n",
    "    yxw = numpy.dot(yx, w).reshape(b, 1) # b-by-1 matrix \n",
    "    vec1 = numpy.exp(-yxw)\n",
    "    vec2 = numpy.log(1 + vec1)\n",
    "    loss = numpy.mean(vec2, axis=0)\n",
    "    reg = lam/2 * numpy.sum(w*w)\n",
    "    obj = loss + reg # b-by-1 matrix \n",
    "    \n",
    "    g_yx = numpy.multiply(yi, xi)\n",
    "    g_yxw = numpy.dot(g_yx, w)\n",
    "    g_vec1 = numpy.exp(g_yxw)\n",
    "    g_vec2 = numpy.divide(-g_yx, 1+g_vec1)\n",
    "    g_vec3 = numpy.mean(g_vec2, axis=0).reshape(d,1)\n",
    "    g_reg = lam * w\n",
    "    g = g_vec3 + g_reg\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Implement mini-batch SGD\n",
    "\n",
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_stochastic_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     b: integer, the batch size\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def mb_sgd(x, y, lam, b, stepsize, max_epoch=100, w=None):\n",
    "    # Fill the function\n",
    "    # Follow the implementation of sgd\n",
    "    # Record one objective value per epoch (not per iteration!)\n",
    "    objvals = numpy.zeros(max_epoch)\n",
    "    n,d = x.shape\n",
    "    \n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1))\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly permute the n samples \n",
    "        random_indicies = numpy.random.permutation(n)\n",
    "        random_x = x[random_indicies, :]\n",
    "        random_y = y[random_indicies, :]\n",
    "        objval = 0\n",
    "         \n",
    "        # Each epoch has n/b iterations\n",
    "        for i in range(int(n/b)):\n",
    "            xi = random_x[i*b:(i+1)*b, :]\n",
    "            yi = random_y[i*b:(i+1)*b, :]\n",
    "            obj, g_w = mb_stochastic_objective_gradient(w, xi, yi, lam, b)\n",
    "            w -= stepsize*g_w\n",
    "            objval += obj\n",
    "        stepsize *= 0.91\n",
    "        objvals[t] = objval * b/n\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objvals[t]))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Run MB-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB-SGD with batch size b=8\n",
    "lam = 1E-6 # do not change\n",
    "b = 8 # do not change\n",
    "stepsize = 0.1 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd8 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MB-SGD with batch size b=64\n",
    "lam = 1E-6 # do not change\n",
    "b = 64 # do not change\n",
    "stepsize = 0.1 # you must tune this parameter\n",
    "\n",
    "w, objvals_mbsgd64 = mb_sgd(x_train, y_train, lam, b, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot and compare GD, SGD, and MB-SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to compare the following algorithms:\n",
    "\n",
    "- Gradient descent (GD)\n",
    "\n",
    "- SGD\n",
    "\n",
    "- MB-SGD with b=8\n",
    "\n",
    "- MB-SGD with b=64\n",
    "\n",
    "Follow the code in Section 4 to plot ```objective function value``` against ```epochs```. There should be four curves in the plot; each curve corresponds to one algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Logistic regression with $\\ell_2$-norm regularization is a strongly convex optimization problem. All the algorithms will converge to the same solution. **In the end, the ``objective function value`` of the 4 algorithms will be the same. If not the same, your implementation must be wrong. Do NOT submit wrong code and wrong result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 4 curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
